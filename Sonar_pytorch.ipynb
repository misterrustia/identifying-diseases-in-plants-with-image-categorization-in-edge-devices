{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8-muXsx0TW-",
        "outputId": "2f955959-563d-49bd-d02b-36d2b82f6488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQqgGmod0PnO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZDVqK8i0PnQ"
      },
      "source": [
        "First we will create a class that would represent the dataset and will have both the training data and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmiUCffp0PnR"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, path, header = 'infer'):\n",
        "        '''\n",
        "        Reads a csv dataset with the assumption that the last column is a categorical label column.\n",
        "        '''\n",
        "        self.df = pd.read_csv(path, header = header)\n",
        "        \n",
        "        self.data = self.df.values[:, :-1]\n",
        "        self.data = self.data.astype('float32')\n",
        "        \n",
        "        self.labels = self.df.values[:, -1]\n",
        "        \n",
        "        # If label is not a number, one-hot encode them\n",
        "        if not np.issubdtype(self.labels.dtype, np.number):\n",
        "            self.label_names = []\n",
        "            for idx, name in enumerate(set(self.labels)):\n",
        "                self.label_names.append(name)\n",
        "                self.labels[self.labels == name] = idx\n",
        "            self.labels = self.labels.astype('float32')\n",
        "            \n",
        "        self.labels = self.labels.reshape(-1, 1)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return (self.data[idx], self.labels[idx])\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return repr(self.df)\n",
        "    \n",
        "    def split_data(self, test_ratio = 0.3):\n",
        "        '''\n",
        "        Splits data into training and test sets.\n",
        "        '''\n",
        "        test_len = round(test_ratio * len(self.data))\n",
        "        train_len = len(self.data) - test_len\n",
        "        \n",
        "        return random_split(self, [train_len, test_len])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcAA39GB0PnS"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\", header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV-0r9VT0PnS",
        "outputId": "372f2ebd-fc65-42a6-8497-7aabb8773dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0       1       2       3       4       5       6       7       8   \\\n",
            "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
            "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
            "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
            "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
            "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
            "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
            "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
            "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
            "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
            "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
            "\n",
            "         9   ...      51      52      53      54      55      56      57  \\\n",
            "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
            "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
            "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
            "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
            "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
            "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
            "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
            "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
            "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
            "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
            "\n",
            "         58      59  60  \n",
            "0    0.0090  0.0032   R  \n",
            "1    0.0052  0.0044   R  \n",
            "2    0.0095  0.0078   R  \n",
            "3    0.0040  0.0117   R  \n",
            "4    0.0107  0.0094   R  \n",
            "..      ...     ...  ..  \n",
            "203  0.0193  0.0157   M  \n",
            "204  0.0062  0.0067   M  \n",
            "205  0.0077  0.0031   M  \n",
            "206  0.0036  0.0048   M  \n",
            "207  0.0061  0.0115   M  \n",
            "\n",
            "[208 rows x 61 columns]\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTZRPef0PnT"
      },
      "source": [
        "We have succesfully read the dataset as a numpy array. We need to convert them into tensors and create batches of data to feed into our neural network.\n",
        "\n",
        "Since the data is already normalized to be between 0 and 1, we do not need to perform any more preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkrhDF8p0PnU"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkdp_Bos0PnV"
      },
      "source": [
        "Before training a full-fledged neural netowork, lets first try using logistic regression to see how it performs on this dataset. We can create a logistic regression uisng PyTorch by defining a single neuron with sigmoid activation.\n",
        "\n",
        "We split the data into 70% training set and 30% test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjI8Sh1t0PnV"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = dataset.split_data(test_ratio = 0.3)\n",
        "\n",
        "# Data Loaders for Logistic Regression\n",
        "train_loader_lr = DataLoader(train_data, batch_size = 1)\n",
        "test_loader_lr = DataLoader(test_data, batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RtDQZxk0PnW"
      },
      "outputs": [],
      "source": [
        "# Defining the number of features available in the dataset\n",
        "N_FEATURES = dataset.data.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbF8oE-t0PnX"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        \n",
        "        # A single neuron\n",
        "        self.neuron = nn.Linear(in_features = N_FEATURES, out_features = 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.neuron(x)\n",
        "        x = torch.sigmoid(x)    # Sigmoid activation on a single neuron, basically makes it logistic regression\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMfEJHhu0PnX"
      },
      "source": [
        "We select L2 loss as our loss function and Stochaistic Gradient descent as optimizer.\n",
        "\n",
        "To train the model, we run a maximum of 100 epochs. If the change in loss between two epochs falls below some tolerence level, we break out of the loop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53cq47230PnX"
      },
      "outputs": [],
      "source": [
        "# Defining loss function and optimizer\n",
        "lr = LogisticRegression()\n",
        "\n",
        "loss_fn_lr = nn.MSELoss()\n",
        "optimizer_lr = optim.SGD(lr.parameters(), lr = 0.001, momentum = 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klVJrANq0PnY"
      },
      "outputs": [],
      "source": [
        "# Training our model\n",
        "loss_hist_lr = []\n",
        "acc_hist_lr = []\n",
        "total_train_size = len(train_loader_lr)\n",
        "TOLERENCE = 0.0001\n",
        "prev_loss = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    curr_loss = 0\n",
        "    correct = 0\n",
        "    for idx, batch in enumerate(train_loader_lr):\n",
        "        input_data, label = batch\n",
        "        \n",
        "        # Forward prop\n",
        "        output = lr(input_data)\n",
        "        loss = loss_fn_lr(output, label)\n",
        "        curr_loss += loss\n",
        "        \n",
        "        # Backprop\n",
        "        optimizer_lr.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_lr.step()\n",
        "        \n",
        "        # Accuracy measure\n",
        "        if torch.round(output) == label:\n",
        "            correct += 1\n",
        "    \n",
        "    avg_loss = curr_loss / total_train_size\n",
        "    accuracy = correct / total_train_size\n",
        "    loss_hist_lr.append(avg_loss)\n",
        "    acc_hist_lr.append(accuracy)\n",
        "    \n",
        "    if abs(avg_loss - prev_loss) < TOLERENCE:\n",
        "        break\n",
        "    else:\n",
        "        prev_loss = avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJeTysBk0PnY",
        "outputId": "6fb69085-f268-47c7-9c61-03b29a02fea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.7877\n",
            "Train loss: 0.1622\n"
          ]
        }
      ],
      "source": [
        "print(\"Train accuracy: {:.4f}\".format(round(acc_hist_lr[-1], 4)))\n",
        "print(\"Train loss: {:.4f}\".format(round(loss_hist_lr[-1].item(), 4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qku5CIGr0Pnc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Sonar pytorch.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}